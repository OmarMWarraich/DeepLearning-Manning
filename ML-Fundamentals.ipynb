{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_validation_samples = 10000\n",
    "np.random.shuffle(data)       #Shuffling the data is usually appropriate\n",
    "validation_data = data[:num_validation_samples]    # Defines the validation set\n",
    "data = data[num_validation_samples:]\n",
    "training_data = data[:]      # Defines the training set\n",
    "model = get_model()\n",
    "model.train(training_data) # Trains a model on the training data & evaluates it on the validation data            \n",
    "validation_score = model.evaluate(validation_data)\n",
    "# At this point, u can tune ur model, retrain it, evaluate it and tune it again\n",
    "\n",
    "mode = get_model()      #Once hyperparameters set, train final model 4rm scratch on all no-test data available.\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "\n",
    "test_score = model.evaluate(test_data)\n",
    "\n",
    "# However, with less or little data available, the validation and test sets may contain 2 few samples 2 b \n",
    "# statistically representative of the data at hand.\n",
    "# K-fold validation and iterated K-fold validation are 2 ways 2 address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#split data into K partitions of equal size.\n",
    "#For each i, train model on remaining K - 1 and evaluate on i. Final score is avg of K scores obtained.\n",
    "\n",
    "k = 4\n",
    "num_validation_samples = len(data) // 4\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    #Selects the validation data partition\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                          num_validation_samples * (fold + 1)]\n",
    "    #Uses the remainder of the data as training data. + sign denotes concatenation.\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "                          data[num_validation_samples * (fold + 1):]\n",
    "        \n",
    "        model = get_model()   # Creates a brand new instance of the model(untrained)\n",
    "        model.train(training_data) \n",
    "        validation_score = model.evaluate(validation_data) \n",
    "        validation_scores.append(validation_score)\n",
    "        \n",
    "    validation_score = np.average(validation_scores)   #validation score = avg of val scores of k folds\n",
    "    \n",
    "    # Trains the final model on all non test data available\n",
    "    model = get_model()\n",
    "    model.train(data)\n",
    "    test_score = model.evaluate(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mind bearers\n",
    "\n",
    "# Data representativeness ---\n",
    "\n",
    "#Training set and test set ought to be representative of the data. e.g, classifying \n",
    "# images of digits, if taking samples in orders of class, then u might take first 80% 5as training containing only\n",
    "# 0-7 classes whereas test set remaining 20% containing classes 8-9. Ridiculous. Avoided by resuffling data \n",
    "# b4 turning into training and test sets.\n",
    "\n",
    "# The arrow of time ---\n",
    "\n",
    "# While predicting furture from past, like weather, stock movements etc., no random shuffling otherwise temporal leak.\n",
    "# i.e., data from training set is leaked into validation. To vaoid that, make sure all data in test set is \n",
    "# posterior to the data in the training set.\n",
    "\n",
    "# Redundancy in your data ---\n",
    "\n",
    "# Make sure that training and validation set are disjoint. otherwise, temporal leak through rendundancy as \n",
    "# data is fairly repeated in real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4 model evaluation, data preprocessing and feature engineering and feature learning arre important.\n",
    "\n",
    "\n",
    "# Data preprocessing for neural networks => makes the raw data more amenable 2 neural networks. includes following: -\n",
    "\n",
    "# Vectorization:-\n",
    "\n",
    "# All inputs or targets must be tensors of floating-point data (or, in specific cases, tensors of integers).\n",
    "# Whatever data u need 2 process -sound, images, text, turn all of it into tensors, process called data vectorization.\n",
    "# For instance, imdb and reuters, we started 4rm text represented as seq of words and we converted 2 tensor of float32\n",
    "# data through one hot encoding. In mnist and boston, data alrdy came in vectorized format, so this step skipd.\n",
    "\n",
    "# Data normalization:-\n",
    "\n",
    "# In mnist, initially image data encoded as integers in 0-255 range(grayscale values). B4 feeding, casted it into\n",
    "# float 32 and divide by 255 for floating point values in 0-1 range. In boston, features with variety of ranges,\n",
    "# some features had small floating point values others had fairly large integer values. Before feeding, each \n",
    "# feature normalized independently so it had a s.d of 1 and mean of 0. To make learning easier 4 network, data :-\n",
    "# ----take small values-typically btw 0-1 range.\n",
    "# ----homogeneous- all features shud take values in roughly the same range.\n",
    "# ----Normalize each feature independenlty to have a mean of 0 and sd of 1.\n",
    "# Easy to do with Numpy arrrays.\n",
    "# x -= x.mean(axis=0)\n",
    "# x /= x.std(axis=0)   Assuming x is a 2D data matrix of shape (samples, features)\n",
    "\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "# Safe to input missing values as 0, the network will learn 4rm exposuure dat value 0 means missing data and shall \n",
    "# learn to ignore the value.\n",
    "# if missing values anticipated in test data ,however training data without missing values, the netwoork wont have \n",
    "# learned to ignore missing values. For this, artificially generate training samples with missing entries. \n",
    "# copy sum trnng smpls frqntly & drp sum feturs expctd 2 b misng in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering;-\n",
    "\n",
    "# using own knowledge abt data and abt mchn lrng algo at hand(neural network here) to make it work better by \n",
    "# applying hardcoded (nonlearned ) transformations to data b4 it goes into the model.\n",
    "# e.g., develop a model dat takes as input an image of the clock and can output the time of the day.\n",
    "\n",
    "# if raw px of imgs taken as input data, difficult problemo. conv neural network den neded to solve it and \n",
    "# lots of comp resrcs to solve it.\n",
    "# However, understanding the problem on higher level(like how humans read time on clock face), can input better \n",
    "# features 4 machine lrnng. i.e., 5 line Py scipt 2 follow the black px of the clock hands & output da (x,y)\n",
    "# coordinates of tip of each hand. A smpl ml algo can learn to associate these coords with the approp time of day.\n",
    "\n",
    "# Further, u cud do a coordinate change, express (x, y) coordinates as polar coordinates with regard 2 the center of \n",
    "# image. input shall becum angle theta of each clock hand. featurization mkng problem easy 2 b smpl round op and \n",
    "# dict lookup no ml reqd. \n",
    "# FE means understanding the prob in depth and expressng in a smpl way.\n",
    "\n",
    "# Good features solve prob elegantly with less resources with farless data. \n",
    "# The ability of DL to learn features on their own relies on having lots of training data availables, if only a few \n",
    "# samples, then info in features become critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERFITTING AND UNDERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fundamental issue in ml is the T(tension) btw optimization & generalization.\n",
    "# Optimization refers 2 adjusting a model 2 get best performance possible on the trenng data(learning in ml), whereas\n",
    "# generalization refers to how well the trained model performs on data it hes neva cen b4. U want good generalization\n",
    "# but cant control it; can only adjust the model besd on trenng data.\n",
    "# Underfitting:_ \n",
    "#initially, optimization & generalization r correlated i.e., lower the loss on training data, lower da loss on\n",
    "# test data. This is underfit model ; there is room 4 improvement. not all relevalant patterns in da trenng data\n",
    "# 've yet been modelled.\n",
    "# Overfitting:-\n",
    "# after certain i's on da trenng data, generalization stops improving and validation metrics stall and den begin \n",
    "# 2 degrade. generalization stops improving& validation metrics stall & den begin degrading. da model is said 2 overfit.\n",
    "# i.e., its begining 2 lern data specific 2 trenng data, but misleading or irrelevant when it comes 2 new data.\n",
    "\n",
    "# How 2 avoid ill(mis)representation;-\n",
    "\n",
    "# The best sol is 2 get more training data. when not possible den modulate the qty of info that model can store or 2 \n",
    "# add constraints on what info its allowed 2 store. R E G U L a R I Z A T I O N : - if network can only memorize a \n",
    "# small no of patterns, the optimization process shall force it 2 focus on da most prominent patterns which 've\n",
    "# a better chance of generalizing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGULARIZATION EGs., (Reducing the network's size)\n",
    "\n",
    "# Prevent overfitting by reducing the size of the model i.e., no of learnable parameters in da model (which is dtd\n",
    "# by no of layers & no of units /layer). In DL, no of learnable paras is often refered to as the model's capacity.\n",
    "# Intuitively, a model with more paras has more memorization capacity & there4 can learn a dict like mappng btw\n",
    "# trenng samples and their targets. a mapping without any generalization power. i.e., a model with 500,000\n",
    "# binary paras cud easily b made 2 learn da class of every digit in Mnist trnng set. we d need only 10 binary paras \n",
    "# for each of the 50,000 digits. but such a model cud be useless 4 classifying new digit samples. \n",
    "# DL model tend 2 b gud at fitting 2 the trenng data but generalization is da goal not fitting.\n",
    "\n",
    "# Conversely, if memorization resources r limited, learning da mapping wont be so easy, however, in order to \n",
    "# minimize its loss, it will have to resort to learning compressed representations that have predictive power\n",
    "# regarding the targets - precisely the type of reps we r interested in. use models having predictive powers regarding\n",
    "# the targets. dont underfit. compromise btw too much capacity and not enuf capacity.\n",
    "\n",
    "# T H E    G E N E R A L   W O R K F L O W\n",
    "\n",
    "# to find an approp model size => start with relatively few layers & parameters, den + size/add layers until\n",
    "# diminishing rtrn wrt validation loss."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# let's try the movie review classification.\n",
    "\n",
    "# Original Model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = model.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Replace with smaller network\n",
    "\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = model.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "the smaller network starts overfitting later than the ref network (six rather than four) & its performance degrades\n",
    "more slowly once it starts overfitting.\n",
    "\n",
    "compare the refrence network vid a 512 layered network.(the bigger network starts overfitting just after one epoch)\n",
    "and overfits much more severly. validation loss is also noisy.\n",
    "\n",
    "the bigger network gets its trainig loss 2 zero very quickly . the more cap the network it can quickly model the \n",
    "trenng data(resulting in lower training loss) but more susceptible to overfitting (resulting in lrg diff btw\n",
    "                                                                                  training and validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A D D I N G   W E I G H T   R E G U L A R I Z A T I O N\n",
    "#Occam's razor\n",
    "# Given 2 exp 4 sth, da simpler one with fewer assumptions most likely 2 be correct. Similarly, for sum trenng data\n",
    "# & a network arch, multi sets of weight values (multi models) cud explain da data. Simpler models r less likely\n",
    "# 2 overfit dan complex ones.A simple model here is one where para dist has less entropy(fewer paras). \n",
    "\n",
    "# Weight Regularization\n",
    "# Commonly overfitting mitigated by putting constraints on a network complexity by forcing its weights to take only\n",
    "# small values, which makes the weight distribution more regular. \n",
    "# its done by adding 2 the loss fn of the network a cost assoc vid hvng lrg weights. This cost has 2 flavors.\n",
    "\n",
    "#. L1 regularization- The cost added is proportional 2 da abs val of the weight coeffs.(da Li norm of the weights).\n",
    "#. L2 regularization- the cost aded is proportional 2 da sq of the val of weight coeefs.(da L2 norm of the weights).\n",
    "# L2 reg also called weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding L2 weight to the movie-review classification network.\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernal_regularizer=regularizers.12(0.001), activation= 'relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernal_regularizer=regularizers.12(0.001), activation= 'relu'))\n",
    "model.add(layers.Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "\n",
    "in the appending figure, the model with Lr regularization dots has become much more resistant to overfitting\n",
    "than the reference model(crosses), even though both have same no of paras.\n",
    "\n",
    "# Different weight regularizers available in keras.\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001)  # L1 regularization\n",
    "\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) # simulataneous l1 and l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout = effective & most commonly used regularization technique. herein, during training, random features are\n",
    "# dropped out(set 2 zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dropout rate is the fraction of the features that are zeroed out. it's usually btw 0.2 & 0.5. No units r \n",
    "# dropped out @ test time., instead, the layer's output values r scaled down by a factor equal 2 da dropout rate 2\n",
    "# balance 4 da fact dat more units r active dan @ training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider, numpy matrix, containing output of a layer, layer_output of shape (batch_size, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At trenng time, we zero out at random a fraction of da values in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_output *= np.random.randint(0, high=2, size=layer_output.shape) # at trenng time, drops out 50% of units in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at test time we scale down the output by the dropout rate. scale by 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_output *= 0.5   # At test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this process can also be done by doing both ops @ trenng time & levng da output unchanged @ test time.\n",
    "\n",
    "# layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "# layer_output /= 0.5       (here we scaling up rthr dan down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 2 Dropout layers in the IMDB network 2 c overfitting reduction."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "figure shows significant improvement over da reference network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following methods to prevent overfitting in neural networks.\n",
    "\n",
    "# Get more training data.\n",
    "# Reduce the capacity of the network.\n",
    "# Add weight regularization.\n",
    "# Add dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Universal Workflow of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the problem and assembling a dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-Data Availibility- \n",
    "-Type of problem-binary classification or multclass. scalar regression or vector. clustering, generation or reinforcement learning.\n",
    "-Identity of problem shall guide to choice of model architecture, loss fn and so on.\n",
    "\n",
    "-Hypothesize outputs based on inputs. and sufficient information available to learn the relationship input-output.\n",
    "\n",
    "However, mere hypotheses wont operate until there is a working model. e.g., predicting a movement of stock based\n",
    "on price history might not give any success coz price history doesnt contain much predictive information.\n",
    "\n",
    "-One class of unsolvable problems is nonstationary problems.\n",
    "e.g., trying to build a recommendation engine for clothing in winter based on training data of august. buying clothes\n",
    "is nonstationary phenomenon over the scale of a few months. A few years data will suffice to apture seasonal variation \n",
    "making time of year an input of ur model.\n",
    "\n",
    "ML can only memorize patterns present in trenng data. U can only recognize what u 've seen before. '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choosing a Measure of Success"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For balanced classification problems, where every class is equally likely, accuracy and area under the reciever\n",
    "operating characterstic curve (ROC AUC) are common metrics. \n",
    "For class-imbalanced problems, u can use precision and recall. \n",
    "For ranking problems, u can use mean avg precision.\n",
    "--Browse the data science competitions on Kaggle(https://kaggle.com); wide range of problems & evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deciding on an Evaluation Protocol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Three common evaluation protocols;-\n",
    "\n",
    "Maintaining a holdout validation set- when plenty of data available.\n",
    "\n",
    "Doing K-fold cross validation- too few samples for hold out validation 2 b reliable.\n",
    "\n",
    "Doing iterated K-fold validation- for performing highly accurate model evaluation when little data available.\n",
    "\n",
    "The first one shall work in most of cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing your data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "format ur data, b4 feeding.\n",
    "\n",
    "- data shud b formatted as tensors.\n",
    "- values taken by these tensors shud usually b scaled 2 small values: e.g., in [-1,1] range or [0,1] range.\n",
    "- if diff features take values in diff ranges (heterogeneous data), then data shud be normalized.\n",
    "- use feature engineering 4 small-data problems.\n",
    "\n",
    "Once tensors of input and target data r redy, begin training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Developing a Model dat does better dan a baseline."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Statistical power is 2 developing a model capable of beating a dumb baseline. \n",
    "\n",
    "\n",
    "Problem type                                      Last-Layer activation                          Loss Function\n",
    "\n",
    "Binary Classification                                     sigmoid                             binary_crossentropy\n",
    "\n",
    "multiclass-singlelabel classification                     softmax                       categorical_crossentropy\n",
    "\n",
    "multiclass-multilabel classification                      sigmoid                             binary_crossentropy\n",
    "\n",
    "Regression to arbitrary values                            None                                mse\n",
    "\n",
    "Regression to values btw 0 and 1                          sigmoid                       mse or binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scaling up: developing a model that overfits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The universal T in ML is btw generalization and optimization; the ideal model is one that stands right at the border\n",
    "btw underfitting and overfitting; btw undercapacity & overcapacity. To figure where this border lies, 1st u mst \n",
    "cross it.\n",
    "\n",
    "develop a model dat overfits. \n",
    "\n",
    "1. Add layers.\n",
    "2. Make the layers bigger.\n",
    "3. Train 4 more epochs.\n",
    "\n",
    "monitor traininloss and validation loss as well as training and validation values for any metrics u care abt. \n",
    "When model's performance on the validation data begins 2 degrade, u have achieved overfitting.\n",
    "\n",
    "next is regularizing and tuning the model to find the ideal model that niether underfits nor overfits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regularizing your model and tuning your hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1- Add dropout.\n",
    "2. Try differenct architectures-add or remove layers.\n",
    "3. Add L1 &/or L2 regularization.\n",
    "4. Try different hyperparameters(such as the number of units per layer or the learning rate of the optimizer)\n",
    "   to find the optimal configuration.\n",
    "5. Optionally, iterate on feature engineering: add new features, or remove features that don't seem 2 b informative.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
